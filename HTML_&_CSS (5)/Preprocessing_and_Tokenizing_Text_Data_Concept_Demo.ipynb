{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beda4d6a",
   "metadata": {
    "id": "beda4d6a"
   },
   "source": [
    "# Demo 1: Preprocessing of Text Data Using Regular Expressions (RegEx)\n",
    "\n",
    "### 1. Remove Numeric Values From Input Text\n",
    "### 2. Remove Extra White Spaces\n",
    "### 3. Replace Symbols and Characters "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5f2a8c",
   "metadata": {
    "id": "5c5f2a8c"
   },
   "source": [
    "## Regular Expressions:\n",
    "A regular expression is a sequence of characters used to find  patterns in a string or file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575e9460",
   "metadata": {
    "id": "575e9460"
   },
   "source": [
    "## Importing Required Libraries and Load Input Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b40b98f",
   "metadata": {
    "id": "2b40b98f"
   },
   "source": [
    "### About Text File \n",
    "#### Gettysburg Address \n",
    "\n",
    "    The Gettysburg Address is a speech that U.S. President Abraham Lincoln delivered during the American Civil War at the dedication of the Soldiers' National Cemetery in Gettysburg, Pennsylvania, on the afternoon of November 19, 1863.\n",
    "\n",
    "    It is one of the best-known speeches in American history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d111bc0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d111bc0b",
    "outputId": "b7e83fad-8e56-4262-de10-c08cf1a79ecb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Four score and seven years ago our fathers brought forth upon this continent, a new nation,\n",
      "\n",
      " conceived in Liberty, and dedicated to the proposition that all men are created equal.\n",
      "Now we are engaged in a great civil war, testing whether that nation, or any nation so\n",
      "\n",
      " conceived and so dedicated, can long endure. We are met on a great battle-field of that \n",
      "\n",
      "war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this.\n",
      "\n",
      "But, in a larger sense, we can not dedicateâ€” we can not consecrateâ€” we can not hallowâ€” this\n",
      " ground. The brave men, living and dead, who struggled here, have consecrated it, far above\n",
      " our poor power to add or detract. The world will little note, nor long remember what we say\n",
      " here, but it can never forget what they did here. \n",
      "\n",
      "\n",
      "66666666666666666666 7777777777777\n",
      "444  222 2222  000\n",
      "\n",
      "It is for us the living, rather, to be \n",
      "dedicated here to the unfinished work which they who fought here have thus far so nobly \n",
      "advanced. \n",
      "\n",
      "\n",
      "\n",
      "It is rather for us to be here dedicated to the great task remaining before \n",
      "usâ€”that from these honored dead we take increased devotion to that cause for which they \n",
      "gave the last full measure of devotionâ€”that we here highly resolve that these dead shall \n",
      "not have died in vainâ€”that this nation, under God, shall have a new birth of freedomâ€”and \n",
      "that government of the people, by the people, for the people, shall not perish from the earth.\n",
      "â€”Abraham Lincoln\n"
     ]
    }
   ],
   "source": [
    "import re                    # Importing re library \n",
    "file1 = open(r\"Gettysburg_Address.txt\", \"r\").read()\n",
    "print(file1)                 # To print contents of input file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5626e8b",
   "metadata": {
    "id": "b5626e8b"
   },
   "source": [
    "### In an input text file, as you can see, there are many unnecessary whitespaces, numerical values, and wrong symbols are present such as â€” which differs from the original speech.\n",
    "### Let's clean the data using RegEx."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102a208c",
   "metadata": {
    "id": "102a208c"
   },
   "source": [
    "# 1. Removal of Numerical Values\n",
    "### In the original speech of Abraham Lincoln, there is no numerical value. But in our input text file, there are numerical values present.\n",
    "### Let's remove those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdf222d5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdf222d5",
    "outputId": "3c5d2684-235b-4bc9-bab8-b66188e9a2f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After removal of Numeric values:\n",
      "\n",
      " Four score and seven years ago our fathers brought forth upon this continent, a new nation,\n",
      "\n",
      " conceived in Liberty, and dedicated to the proposition that all men are created equal.\n",
      "Now we are engaged in a great civil war, testing whether that nation, or any nation so\n",
      "\n",
      " conceived and so dedicated, can long endure. We are met on a great battle-field of that \n",
      "\n",
      "war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this.\n",
      "\n",
      "But, in a larger sense, we can not dedicateâ€” we can not consecrateâ€” we can not hallowâ€” this\n",
      " ground. The brave men, living and dead, who struggled here, have consecrated it, far above\n",
      " our poor power to add or detract. The world will little note, nor long remember what we say\n",
      " here, but it can never forget what they did here. \n",
      "\n",
      "\n",
      " \n",
      "     \n",
      "\n",
      "It is for us the living, rather, to be \n",
      "dedicated here to the unfinished work which they who fought here have thus far so nobly \n",
      "advanced. \n",
      "\n",
      "\n",
      "\n",
      "It is rather for us to be here dedicated to the great task remaining before \n",
      "usâ€”that from these honored dead we take increased devotion to that cause for which they \n",
      "gave the last full measure of devotionâ€”that we here highly resolve that these dead shall \n",
      "not have died in vainâ€”that this nation, under God, shall have a new birth of freedomâ€”and \n",
      "that government of the people, by the people, for the people, shall not perish from the earth.\n",
      "â€”Abraham Lincoln\n"
     ]
    }
   ],
   "source": [
    "mod_string = ''.join(filter(lambda item: not item.isdigit(), file1))         # Filter all digits from characters in string & join remaining character\n",
    "print('\\nAfter removal of Numeric values:\\n\\n',mod_string)                   # print text after removal of Numeric values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca62d189",
   "metadata": {
    "id": "ca62d189"
   },
   "source": [
    "## 2. Remove Extra White Spaces so there is only one space between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc9f51b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc9f51b7",
    "outputId": "d7da2e13-7812-4154-fde2-2fbec561e4d7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input TEXT:\n",
      "\n",
      " Four score and seven years ago our fathers brought forth upon this continent, a new nation,\n",
      "\n",
      " conceived in Liberty, and dedicated to the proposition that all men are created equal.\n",
      "Now we are engaged in a great civil war, testing whether that nation, or any nation so\n",
      "\n",
      " conceived and so dedicated, can long endure. We are met on a great battle-field of that \n",
      "\n",
      "war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this.\n",
      "\n",
      "But, in a larger sense, we can not dedicateâ€” we can not consecrateâ€” we can not hallowâ€” this\n",
      " ground. The brave men, living and dead, who struggled here, have consecrated it, far above\n",
      " our poor power to add or detract. The world will little note, nor long remember what we say\n",
      " here, but it can never forget what they did here. \n",
      "\n",
      "\n",
      " \n",
      "     \n",
      "\n",
      "It is for us the living, rather, to be \n",
      "dedicated here to the unfinished work which they who fought here have thus far so nobly \n",
      "advanced. \n",
      "\n",
      "\n",
      "\n",
      "It is rather for us to be here dedicated to the great task remaining before \n",
      "usâ€”that from these honored dead we take increased devotion to that cause for which they \n",
      "gave the last full measure of devotionâ€”that we here highly resolve that these dead shall \n",
      "not have died in vainâ€”that this nation, under God, shall have a new birth of freedomâ€”and \n",
      "that government of the people, by the people, for the people, shall not perish from the earth.\n",
      "â€”Abraham Lincoln\n",
      "\n",
      " \n",
      "  After REMOVING extra White Spaces:\n",
      "\n",
      " Four score and seven years ago our fathers brought forth upon this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We are met on a great battle-field of that war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this. But, in a larger sense, we can not dedicateâ€” we can not consecrateâ€” we can not hallowâ€” this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for us to be here dedicated to the great task remaining before usâ€”that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotionâ€”that we here highly resolve that these dead shall not have died in vainâ€”that this nation, under God, shall have a new birth of freedomâ€”and that government of the people, by the people, for the people, shall not perish from the earth. â€”Abraham Lincoln\n"
     ]
    }
   ],
   "source": [
    "item = mod_string                                                   # input file\n",
    "text_new = re.sub(r'\\s+',' ', item)                                 # Replacing extra white space by a single space\n",
    "print('Input TEXT:\\n\\n',item)                                       # To print text before removing symbols\n",
    "print('\\n \\n  After REMOVING extra White Spaces:\\n\\n',text_new)     # To print text after removing symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e101a6d7",
   "metadata": {
    "id": "e101a6d7"
   },
   "source": [
    "### As you can compare with input text, how compact the speech has become after the removal of extra white spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a010fad3",
   "metadata": {
    "id": "a010fad3"
   },
   "source": [
    "# 3. a) Replacing Symbols and Characters\n",
    "### In an original speech of Abraham Lincoln, — the symbol has been used, but erroneous symbols â€” are present in our input text file.\n",
    "#### Let's replace the symbol â€” with the right symbol — "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f42c026",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f42c026",
    "outputId": "1beeedcb-018e-4b94-e80a-7015e05a7c3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Replacing symbol:\n",
      "\n",
      " Four score and seven years ago our fathers brought forth upon this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We are met on a great battle-field of that war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this. But, in a larger sense, we can not dedicate— we can not consecrate— we can not hallow— this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for us to be here dedicated to the great task remaining before us—that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion—that we here highly resolve that these dead shall not have died in vain—that this nation, under God, shall have a new birth of freedom—and that government of the people, by the people, for the people, shall not perish from the earth. —Abraham Lincoln\n"
     ]
    }
   ],
   "source": [
    "sample_string = text_new                                       # Taking text with removed whitespaces as input here                                                 \n",
    "#print('Original text:',sample_string)                         # print original text\n",
    "char_to_replace = {'â€”': '—'}                                 # Define characters to be replaced                                \n",
    "for key, value in char_to_replace.items():                     # Iterate over all key-value pairs in dictionary\n",
    "    sample_string = sample_string.replace(key, value)          # Replace key character with value character in string\n",
    "print('After Replacing symbol:\\n\\n',sample_string)             # print text after replacing characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d7f35f",
   "metadata": {
    "id": "30d7f35f"
   },
   "source": [
    "#### You can try to replace other symbols and characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba9488b",
   "metadata": {
    "id": "3ba9488b"
   },
   "source": [
    "# 3. b) Removing Symbols and Characters From Text\n",
    "#### Let's assume another scenario where HR wants to calculate the total salary of a few employees for some analysis. \n",
    "#### But there is a dollar symbol before each number due to which it is difficult to calculate the sum. \n",
    "#### Let's help him to remove the dollar sign and perform the sum of those employees' salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a290f20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3a290f20",
    "outputId": "6972575e-7337-49ea-d963-2e0c3df32d3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original TEXT:\n",
      "\n",
      " $1000,$2000,$3000,$4000,$100,$200,$300,$400\n",
      "\n",
      " \n",
      "After REMOVING Dollar symbol for easy sum of salaries:\n",
      "\n",
      "  1000  2000  3000  4000  100  200  300  400\n"
     ]
    }
   ],
   "source": [
    "strs = \"$1000,$2000,$3000,$4000,$100,$200,$300,$400\"                              # Input string\n",
    "nstr = re.sub('[$|,]',' ',strs)                                                   # Here the pattern '[$|,]' indicate that our reggex pattern contain the symbole '$', or ','\n",
    "print('Original TEXT:\\n\\n',strs)                                                  # To print text before removing symbols\n",
    "print('\\n \\nAfter REMOVING Dollar symbol for easy sum of salaries:\\n\\n',nstr)     # To print text after removing symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97fdedb",
   "metadata": {
    "id": "a97fdedb"
   },
   "source": [
    "# Demo 2: spaCy Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b445ef1",
   "metadata": {
    "id": "0b445ef1"
   },
   "source": [
    "## Check if pip is installed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa53c00",
   "metadata": {
    "id": "fb3c2814"
   },
   "source": [
    "Open command prompt    \n",
    "pip –version  \n",
    "pip 21.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69895473",
   "metadata": {
    "id": "69895473"
   },
   "source": [
    "## Update pip, setuptools, wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e324d9a9",
   "metadata": {
    "id": "8f8a0a64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (22.0.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (62.1.0)\n",
      "Requirement already satisfied: wheel in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.37.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U pip setuptools wheel "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb8166f",
   "metadata": {
    "id": "bbb8166f"
   },
   "source": [
    "## Installation of spaCy in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5d0279a",
   "metadata": {
    "id": "5ef0c20a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.3.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.7.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (62.1.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (8.0.15)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.9.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.1.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (1.22.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (2.4.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8197771",
   "metadata": {
    "id": "a8197771"
   },
   "source": [
    "## Downloading Specific Model for spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "614b5362",
   "metadata": {
    "id": "6cce4167"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 5.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.22.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.1.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (62.1.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.15)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.27.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.26.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.3.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d3f408",
   "metadata": {
    "id": "61d3f408"
   },
   "source": [
    "## Importing Library and Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d81e8f49",
   "metadata": {
    "id": "d81e8f49"
   },
   "outputs": [],
   "source": [
    "import spacy                                  # Import spaCy library\n",
    "nlp = spacy.load(\"en_core_web_sm\")            # load spaCy model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afe19ca",
   "metadata": {
    "id": "9afe19ca"
   },
   "source": [
    "# Demo 3\n",
    "### How to open and write text files using spaCy\n",
    "### Pipeline components\n",
    "### Step1: Create an NLP object\n",
    "### Step2: Tokenization \n",
    "### Frequency word count\n",
    "### Most common words\n",
    "\n",
    "## Let's get started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f569436",
   "metadata": {
    "id": "2f569436"
   },
   "source": [
    "# To open text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "491e43e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "491e43e1",
    "outputId": "ea4dffa6-f2bc-4138-9bad-e1613a8f41fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rama eats apple\n"
     ]
    }
   ],
   "source": [
    "file_name = 'Review1.txt'                                       # File name\n",
    "introduction_file_text = open(file_name).read()                 # To open text file\n",
    "\n",
    "#CONVERTING TO NLP OBJECT/TYPE TO APPLY spaCy on it!\n",
    "introduction_file_doc = nlp(introduction_file_text)             # Create NLP object\n",
    "\n",
    "print(introduction_file_doc)                                    # To print contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e928a12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rama eats apple\n",
      "\n",
      "He plays cricket.\n"
     ]
    }
   ],
   "source": [
    "file_name = 'Review2.txt'                                       # File name\n",
    "introduction_file_text = open(file_name).read()                 # To open text file\n",
    "\n",
    "#CONVERTING TO NLP OBJECT/TYPE TO APPLY spaCy on it!\n",
    "introduction_file_doc = nlp(introduction_file_text)             # Create NLP object\n",
    "\n",
    "print(introduction_file_doc)                                    # To print contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617708ea",
   "metadata": {
    "id": "617708ea"
   },
   "source": [
    "## Read a file line by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31281831",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31281831",
    "outputId": "5a388eac-49af-4388-b3d6-949380839aa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rama eats apple\\n', '\\n', 'He plays cricket.']\n"
     ]
    }
   ],
   "source": [
    "myfile = open('Review2.txt')                                    # To open a file\n",
    "print(myfile.readlines())                                       # To read file line by line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc238c22",
   "metadata": {
    "id": "cc238c22"
   },
   "source": [
    "# To write text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "825b68b9",
   "metadata": {
    "id": "825b68b9"
   },
   "outputs": [],
   "source": [
    "file = open('New.txt', 'w')                                     # To write New text file\n",
    "file.write('Created new file. using above steps!') # It gives number of characters in a sentence\n",
    "file.close()                                                    # To update new created file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0d6e18",
   "metadata": {
    "id": "da0d6e18"
   },
   "source": [
    "### A new text file is created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755016eb",
   "metadata": {
    "id": "755016eb"
   },
   "source": [
    "# Pipeline Components\n",
    "To figure out the active pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb1cf91a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb1cf91a",
    "outputId": "4f646bd4-68a8-47a2-9127-33665ef7745b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names                                # To display active pipeline components:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19fbd8f",
   "metadata": {
    "id": "d19fbd8f"
   },
   "source": [
    "To disable the pipeline components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3098d77b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3098d77b",
    "outputId": "64edfc10-7cb7-404a-9518-c882583a3782"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.disable_pipes('tagger', 'parser')          # to disable the pipeline components\n",
    "nlp.pipe_names                                 # Again check active pipeline components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ed8145",
   "metadata": {
    "id": "19ed8145"
   },
   "source": [
    "### Note: All the pipeline components will be discussed in upcoming sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1124859",
   "metadata": {
    "id": "c1124859"
   },
   "source": [
    "# Step1: Create an NLP object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6662e6de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6662e6de",
    "outputId": "10b4e90c-e15f-499b-8e7d-920796c0b82b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text1: Mahira goes to school daily.\n",
      "\n",
      "Text2: In academic writing, readers expect each paragraph to have a sentence or two that captures its main point. They’re often called “topic sentences,” though many writing instructors prefer to call them key sentences.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Mahira goes to school daily.\")     # Text as NLP object\n",
    "print('Text1:',doc)                           # To print the text\n",
    "doc2 = nlp(\"In academic writing, readers expect each paragraph to have a sentence or two that captures its main point. They’re often called “topic sentences,” though many writing instructors prefer to call them key sentences.\")   # Text as NLP object\n",
    "print('\\nText2:',doc2)                        # To print the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5660409b",
   "metadata": {
    "id": "5660409b"
   },
   "source": [
    "# Step2: Tokenization\n",
    " \n",
    "## For the single input sentence \"Rama eats apple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5191ffe7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5191ffe7",
    "outputId": "19296a3d-6cb3-4f77-e7c0-a61f9983821f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mahira', 'goes', 'to', 'school', 'daily', '.']\n"
     ]
    }
   ],
   "source": [
    "print ([token.text for token in doc])        # Extract tokens for the given doc...\n",
    "                                             #(fullstop is also a token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2fb780c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'academic', 'writing', ',', 'readers', 'expect', 'each', 'paragraph', 'to', 'have', 'a', 'sentence', 'or', 'two', 'that', 'captures', 'its', 'main', 'point', '.', 'They', '’re', 'often', 'called', '“', 'topic', 'sentences', ',', '”', 'though', 'many', 'writing', 'instructors', 'prefer', 'to', 'call', 'them', 'key', 'sentences', '.']\n"
     ]
    }
   ],
   "source": [
    "print([token.text for token in doc2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce81506",
   "metadata": {
    "id": "2ce81506"
   },
   "source": [
    "## For Raw Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38d7c100",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38d7c100",
    "outputId": "ed872b49-44ca-49aa-dfe6-afaebdda08ab",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               TEN THINGS I HATE ABOUT YOU\n",
      "          \n",
      "                written by Karen McCullah Lutz & Kirsten Smith\n",
      "          \n",
      "              based on 'Taming of the Shrew\" by William Shakespeare\n",
      "          \n",
      "          Revision November 12, 1997\n",
      "          \n",
      "          \n",
      "          PADUA HIGH SCHOOL - DAY\n",
      "          \n",
      "          Welcome to Padua High School,, your typical urban-suburban \n",
      "          high school in Portland, Oregon.  Smarties, Skids, Preppies, \n",
      "          Granolas. Loners, Lov\n",
      "                               \n",
      "TEN\n",
      "THINGS\n",
      "I\n",
      "HATE\n",
      "ABOUT\n",
      "YOU\n",
      "\n",
      "          \n",
      "                \n",
      "written\n",
      "by\n",
      "Karen\n",
      "McCullah\n",
      "Lutz\n",
      "&\n",
      "Kirsten\n",
      "Smith\n",
      "\n",
      "          \n",
      "              \n",
      "based\n",
      "on\n",
      "'\n",
      "Taming\n",
      "of\n",
      "the\n",
      "Shrew\n",
      "\"\n",
      "by\n",
      "William\n",
      "Shakespeare\n",
      "\n",
      "          \n",
      "          \n",
      "Revision\n",
      "November\n",
      "12\n",
      ",\n",
      "1997\n",
      "\n",
      "          \n",
      "          \n",
      "          \n",
      "PADUA\n",
      "HIGH\n",
      "SCHOOL\n",
      "-\n",
      "DAY\n",
      "\n",
      "          \n",
      "          \n",
      "Welcome\n",
      "to\n",
      "Padua\n",
      "High\n",
      "School\n",
      ",\n",
      ",\n",
      "your\n",
      "typical\n",
      "urban\n",
      "-\n",
      "suburban\n",
      "\n",
      "          \n",
      "high\n",
      "school\n",
      "in\n",
      "Portland\n",
      ",\n",
      "Oregon\n",
      ".\n",
      " \n",
      "Smarties\n",
      ",\n",
      "Skids\n",
      ",\n",
      "Preppies\n",
      ",\n",
      "\n",
      "          \n",
      "Granolas\n",
      ".\n",
      "Loners\n",
      ",\n",
      "Lov\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English                              # Importing library\n",
    "nlp = English()                                                # Importing model\n",
    "nlp = spacy.load(\"en_core_web_sm\")                             # Importing model\n",
    "f = open('Ten_things_I_hate.txt')\n",
    "contents = f.read()                                            # To read input dataset\n",
    "contents = contents[:500]                                      # First few characters of input file\n",
    "print(contents)                                                # To print contents of file\n",
    "text_combined = str(contents)                                  # String\n",
    "doc = nlp(text_combined)                                       # Create NLP object\n",
    "for token in doc:\n",
    "    print(token)                                               # Print tokens\n",
    "len(token)                                                     # Length of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3ae5d3",
   "metadata": {
    "id": "6b3ae5d3"
   },
   "source": [
    "# Frequency of Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81d87055",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81d87055",
    "outputId": "63b6ff71-7f88-40a4-ddaa-1c3930455a6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({',': 8, 'by': 2, '\\n          \\n          ': 2, '-': 2, '\\n          ': 2, '.': 2, '                               ': 1, 'TEN': 1, 'THINGS': 1, 'I': 1, 'HATE': 1, 'ABOUT': 1, 'YOU': 1, '\\n          \\n                ': 1, 'written': 1, 'Karen': 1, 'McCullah': 1, 'Lutz': 1, '&': 1, 'Kirsten': 1, 'Smith': 1, '\\n          \\n              ': 1, 'based': 1, 'on': 1, \"'\": 1, 'Taming': 1, 'of': 1, 'the': 1, 'Shrew': 1, '\"': 1, 'William': 1, 'Shakespeare': 1, 'Revision': 1, 'November': 1, '12': 1, '1997': 1, '\\n          \\n          \\n          ': 1, 'PADUA': 1, 'HIGH': 1, 'SCHOOL': 1, 'DAY': 1, 'Welcome': 1, 'to': 1, 'Padua': 1, 'High': 1, 'School': 1, 'your': 1, 'typical': 1, 'urban': 1, 'suburban': 1, 'high': 1, 'school': 1, 'in': 1, 'Portland': 1, 'Oregon': 1, ' ': 1, 'Smarties': 1, 'Skids': 1, 'Preppies': 1, 'Granolas': 1, 'Loners': 1, 'Lov': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counts = Counter()\n",
    "for token in doc:\n",
    "    counts[token.orth_] += 1                       # Equivalently, token.text\n",
    "print(counts)                                      # To print the frequency count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b1394f",
   "metadata": {
    "id": "52b1394f"
   },
   "source": [
    "# Most Common Words in Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "640a5e09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "640a5e09",
    "outputId": "bfad03ba-65f0-4829-a3ab-a943e37a7279"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 22),\n",
       " ('that', 13),\n",
       " ('.', 10),\n",
       " ('the', 9),\n",
       " ('to', 8),\n",
       " ('we', 8),\n",
       " ('here', 8),\n",
       " ('—', 8),\n",
       " ('a', 7),\n",
       " ('and', 6),\n",
       " ('nation', 5),\n",
       " ('can', 5),\n",
       " ('of', 5),\n",
       " ('have', 5),\n",
       " ('for', 5)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = str(sample_string)                             # Lincoln speech as Input string\n",
    "doc = nlp(text)                                       # Create NLP object\n",
    "from collections import Counter\n",
    "counts = Counter()                                    # Frequency count\n",
    "for token in doc:\n",
    "    counts[token.orth_] += 1                          # Equivalently, token.text\n",
    "m = counts.most_common(15)                            # Most common words in document\n",
    "m                                                     # To print most common words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5f22ee",
   "metadata": {
    "id": "7a5f22ee"
   },
   "source": [
    "### Here, we can see that the most common words are 'that', 'the', 'we', 'here' etc.\n",
    "### These should be removed for better text analysis.\n",
    "### In the next sprint, we will remove such stop words using spaCy."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "02_DS2_C1_S5_Preprocessing and Tokenizing Text Data_Concept Session_Demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
