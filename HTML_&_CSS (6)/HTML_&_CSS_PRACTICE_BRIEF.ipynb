{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f9ef527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy                                            \n",
    "from spacy.lang.en import English\n",
    "import nltk                                   \n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a92019ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5dd0cd",
   "metadata": {},
   "source": [
    "# TASK 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ed3360",
   "metadata": {},
   "source": [
    "##### Printing no. of  stop-words consists in spaCy and from it displaying first 15 of them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f30c855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First fifteen stop words of spaCy :\n",
      " ['she', 'hereafter', 'front', 'and', 'everyone', 'with', 'three', 'yet', 'done', 'other', 'elsewhere', 'your', 'twenty', 'into', 'either']\n",
      "Number of stop words in spaCy : 326\n"
     ]
    }
   ],
   "source": [
    "spaCy_stopwords = spacy.lang.en.stop_words.STOP_WORDS \n",
    "\n",
    "print('First fifteen stop words of spaCy :\\n %s' % list(spaCy_stopwords)[:15]) \n",
    "\n",
    "print('Number of stop words in spaCy : %d' % len(spaCy_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e890c6cd",
   "metadata": {},
   "source": [
    "# TASK 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1acf75",
   "metadata": {},
   "source": [
    "##### Stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c095493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cries ----- cri\n",
      "lied ----- lie\n",
      "this ----- this\n",
      "computing ----- comput\n",
      "organizing ----- organ\n",
      "matches ----- match\n"
     ]
    }
   ],
   "source": [
    "stemmer=SnowballStemmer(language=\"english\")\n",
    "token=[\"cries\", \"lied\", \"this\", \"computing\", \"organizing\", \"matches\"]\n",
    "\n",
    "for tokens in token:\n",
    "    print(tokens+\" ----- \"+stemmer.stem(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a7acd1",
   "metadata": {},
   "source": [
    "##### Lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8044cf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cries ----- cry\n",
      "lied ----- lie\n",
      "this ----- this\n",
      "computing ----- compute\n",
      "organizing ----- organizing\n",
      "matches ----- match\n"
     ]
    }
   ],
   "source": [
    "token=nlp(\"cries lied this computing organizing matches\")\n",
    "\n",
    "for word in token:                                                      \n",
    "    print(word.text,\"-----\",word.lemma_)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7bbe59",
   "metadata": {},
   "source": [
    "##### Comparing Stemming vs Lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "edc8b6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BY STEMMING:\n",
      "\n",
      "\n",
      "cries ----- cri\n",
      "lied ----- lie\n",
      "this ----- this\n",
      "computing ----- comput\n",
      "organizing ----- organ\n",
      "matches ----- match\n",
      "\n",
      "\n",
      "BY LEMMATIZATION:\n",
      "\n",
      "\n",
      "cries ----- cry\n",
      "lied ----- lie\n",
      "this ----- this\n",
      "computing ----- compute\n",
      "organizing ----- organizing\n",
      "matches ----- match\n"
     ]
    }
   ],
   "source": [
    "print(\"BY STEMMING:\")\n",
    "print(\"\\n\")\n",
    "stemmer=SnowballStemmer(language=\"english\")\n",
    "token=[\"cries\", \"lied\", \"this\", \"computing\", \"organizing\", \"matches\"]\n",
    "\n",
    "for tokens in token:\n",
    "    print(tokens+\" ----- \"+stemmer.stem(tokens))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"BY LEMMATIZATION:\")\n",
    "print(\"\\n\")\n",
    "token=nlp(\"cries lied this computing organizing matches\")\n",
    "\n",
    "for word in token:                                                      \n",
    "    print(word.text,\"-----\",word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c97db29",
   "metadata": {},
   "source": [
    "##### We can observe that the Lemmatization is more precise in giving the output for the words in their root form compared to Stemming...!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d73f8f",
   "metadata": {},
   "source": [
    "# TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0b031ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"scifiscripts_intro.txt\")\n",
    "contents=file.read()\n",
    "text=str(contents)\n",
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6cfaf51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \t Whole file after removing stop-words is : \n",
      "\n",
      "  [Note, poster, Kubrick, newsgroup, :, \n",
      "\n",
      ", found, bbs, ago, thought, pass, \n",
      ", Kubrick, freaks, ., \n",
      "\n",
      ", 02/23/89, \n",
      ", Transcriber, note, :, \n",
      "\n",
      ", Clarke, /, Kubrick/2001, fans, ,, \n",
      "\n",
      ", found, original, paper, copy, screenplay, felt, \n",
      ", compelled, transcribe, disk, upload, bulletin, \n",
      ", boards, enjoyment, ., \n",
      "\n",
      ", final, movie, deviates, screenplay, number, interesting, \n",
      ", ways, ., tried, maintain, format, original, document, \n",
      ", number, lines, page, original, ., order, reduce, \n",
      ", length, file, bar, \", ------, \", delimit, pages, \n",
      ", lot, whitespace, original, screenplay, page, .]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stop_word_file=[]\n",
    "\n",
    "for i in doc:\n",
    "    if i.is_stop==False:\n",
    "        stop_word_file.append(i)\n",
    "print(\"\\n \\t Whole file after removing stop-words is : \\n\\n \",stop_word_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8af4fe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=(\" My mom taught me to finish everything on my plate at dinner.\\\n",
    "The only problem with a pencil, is that they do not stay sharp long enough.\\\n",
    "Our school building is made of bricks.\\\n",
    "Every night I get woken up by the sound of a barking dog across the street.\\\n",
    "Salad is for rabbits. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3c7a4499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \t Whole file after removing stop-words is : \n",
      "\n",
      "  [ , mom, taught, finish, plate, dinner, ., problem, pencil, ,, stay, sharp, long, ., school, building, bricks, ., night, woken, sound, barking, dog, street, ., Salad, rabbits, .]\n"
     ]
    }
   ],
   "source": [
    "str_file=str(text)\n",
    "document=nlp(str_file)\n",
    "\n",
    "stop_word_file2=[]\n",
    "\n",
    "for i in document:\n",
    "    if i.is_stop==False:\n",
    "        stop_word_file2.append(i)\n",
    "\n",
    "print(\"\\n \\t Whole file after removing stop-words is : \\n\\n \",stop_word_file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc6c059",
   "metadata": {},
   "source": [
    "# TASK 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c15fa988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bryan \t PROPN \t NNP\n",
      "visited \t VERB \t VBD\n",
      "his \t PRON \t PRP$\n",
      "friend \t NOUN \t NN\n",
      "for \t ADP \t IN\n",
      "a \t DET \t DT\n",
      "while \t NOUN \t NN\n",
      "and \t CCONJ \t CC\n",
      "then \t ADV \t RB\n",
      "went \t VERB \t VBD\n",
      "home \t ADV \t RB\n",
      "at \t ADP \t IN\n",
      "10 \t NUM \t CD\n",
      "pm \t NOUN \t NN\n",
      ". \t PUNCT \t .\n"
     ]
    }
   ],
   "source": [
    "sentence=nlp(\"Bryan visited his friend for a while and then went home at 10 pm.\")\n",
    "\n",
    "for word in sentence:\n",
    "    print(word.text,'\\t',word.pos_,'\\t',word.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f56cdb",
   "metadata": {},
   "source": [
    "# TASK 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43471250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PADUA \t PROPN \t NNP\n",
      "HIGH \t PROPN \t NNP\n",
      "SCHOOL \t PROPN \t NNP\n",
      "- \t PUNCT \t HYPH\n",
      "DAY \t PROPN \t NNP\n",
      "\n",
      " \t SPACE \t _SP\n",
      "Revision \t PROPN \t NNP\n",
      "November \t PROPN \t NNP\n",
      "12 \t NUM \t CD\n",
      ", \t PUNCT \t ,\n",
      "1997 \t NUM \t CD\n",
      "\n",
      " \t SPACE \t _SP\n"
     ]
    }
   ],
   "source": [
    "file = open('Random.txt')\n",
    "content = file.read()                                                  \n",
    "text = str(content)                                               \n",
    "doc = nlp(text) \n",
    "\n",
    "for word in doc:                                            \n",
    "    print(word.text,'\\t',word.pos_,'\\t',word.tag_)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bfeee3",
   "metadata": {},
   "source": [
    "* Proper Noun:  \n",
    ">PADUA  \n",
    "HIGH  \n",
    "SCHOOL  \n",
    "DAY  \n",
    "REVISION  \n",
    "November    \n",
    "\n",
    "* Numericals:\n",
    ">12  \n",
    "1997"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a152ff",
   "metadata": {},
   "source": [
    "# TASK 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f343171",
   "metadata": {},
   "source": [
    "##### Adding 5 new stop-words..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5029737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'she', 'hereafter', 'front', 'and', 'everyone', 'with', 'three', 'yet', 'done', 'other', 'elsewhere', 'your', 'twenty', 'into', 'either', 'did', 'from', 'anything', 'its', 'became', 'now', 'whole', 'then', 'can', 'whereafter', 'put', 'some', 'regarding', 'him', \"'s\", 'latterly', 'though', 'almost', 'her', 'on', 'STOPWORD1', 'no', 'n’t', 'everything', 'whereby', 'ten', 'be', 'me', 'noone', 'i', 'of', 'whom', 'others', 'nevertheless', 'themselves', 'namely', 'really', 'many', 'afterwards', 'beforehand', 'how', 'keep', 'part', 'an', 'even', 'too', 'towards', 'therefore', 'could', 'you', 'although', 'whence', 'may', 'less', 'again', 'after', 'STEPWORD5', 'if', 're', 'someone', 'already', 'alone', 'the', 'make', 'below', 'but', 'ever', 'nor', \"'re\", 'yours', 'must', 'without', 'whether', 'wherever', 'becomes', 'bottom', 'upon', 'mostly', 'formerly', 'among', 'seeming', 'hundred', 'whereupon', 'my', 'there', 'go', 'used', 'serious', 'show', 'anyway', 'last', 'is', 'herein', 'amongst', 'wherein', 'same', 'thereupon', 'somehow', 'become', 'under', '‘m', '’re', 'through', 'first', 'n‘t', 'seem', 'meanwhile', 'never', 'forty', 'STEPWORD3', 'not', 'perhaps', 'two', 'former', 'those', 'doing', 'over', \"'d\", 'behind', 'back', 'sixty', 'none', 'ca', \"'m\", 'somewhere', 'these', 'more', 'it', 'himself', 'have', 'we', 'off', 'do', 'full', 'any', 'becoming', 'however', 'itself', 'one', 'which', 'enough', 'anyhow', 'as', 'rather', 'move', 'nobody', 'made', 'third', 'anywhere', 'well', 'various', 'six', 'was', 'against', 'several', 'moreover', 'get', 'thus', 'often', 'whatever', 'always', 'between', 'whereas', 'hers', 'than', 'empty', 'due', '’ve', 'cannot', 'indeed', 'above', 'name', 'thereby', 'our', 'top', 'when', 'every', 'also', 'am', 'thereafter', 'here', 'unless', 'herself', 'just', 'all', 'still', 'something', 'together', 'otherwise', 'a', 'them', 'anyone', 'along', 'sometime', 'using', 'seemed', 'twelve', 'quite', 'beside', 'own', 'so', 'onto', 'another', 'myself', 'see', 'most', 'next', 'are', 'why', 'around', 'their', 'side', 'would', 'STEPWORD4', '‘d', 'therein', 'who', 'across', 'hereupon', 'before', 'five', 'nine', 'at', 'STEPWORD2', 'everywhere', 'mine', 'because', 'latter', 'such', 'by', 'four', 'while', 'much', 'else', 'fifteen', 'ours', 'very', '’d', 'further', 'only', \"'ve\", 'nothing', 'say', 'toward', 'throughout', 'within', \"n't\", 'eleven', 'via', 'ourselves', '‘re', '‘s', 'nowhere', 'or', '‘ll', 'whose', 'sometimes', 'call', 'yourself', '‘ve', 'might', 'out', 'about', 'least', 'in', 'should', 'give', 'will', 'few', 'what', 'they', 'yourselves', 'both', 'except', 'to', 'been', 'where', 'being', 'were', 'once', 'besides', 'does', 'amount', 'beyond', 'neither', 'this', 'hereby', '’m', 'eight', '’s', 'for', 'has', 'each', 'us', 'thru', 'hence', 'per', 'whenever', 'he', 'seems', '’ll', 'whoever', 'since', 'fifty', 'whither', 'had', 'thence', 'please', 'that', 'up', 'down', 'during', 'until', 'his', 'take', \"'ll\"}\n"
     ]
    }
   ],
   "source": [
    "nlp.Defaults.stop_words |= {\"STOPWORD1\",\"STEPWORD2\",\"STEPWORD3\",\"STEPWORD4\",\"STEPWORD5\"}\n",
    "print(nlp.Defaults.stop_words)                             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ff509",
   "metadata": {},
   "source": [
    "##### Removing 4 stop-words..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2baf6809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'she', 'hereafter', 'front', 'and', 'everyone', 'with', 'three', 'yet', 'done', 'other', 'elsewhere', 'your', 'twenty', 'into', 'either', 'did', 'from', 'anything', 'its', 'became', 'now', 'whole', 'then', 'can', 'whereafter', 'put', 'some', 'regarding', 'him', \"'s\", 'latterly', 'though', 'almost', 'her', 'on', 'STOPWORD1', 'no', 'n’t', 'everything', 'whereby', 'ten', 'be', 'me', 'noone', 'i', 'of', 'whom', 'others', 'nevertheless', 'themselves', 'namely', 'really', 'many', 'afterwards', 'beforehand', 'how', 'keep', 'part', 'an', 'even', 'too', 'towards', 'therefore', 'could', 'you', 'although', 'whence', 'may', 'less', 'again', 'after', 'STEPWORD5', 'if', 're', 'someone', 'already', 'alone', 'the', 'make', 'below', 'but', 'ever', 'nor', \"'re\", 'yours', 'must', 'without', 'whether', 'wherever', 'bottom', 'upon', 'mostly', 'formerly', 'among', 'seeming', 'hundred', 'whereupon', 'my', 'there', 'go', 'used', 'serious', 'show', 'anyway', 'last', 'is', 'herein', 'amongst', 'wherein', 'same', 'thereupon', 'somehow', 'become', 'under', '‘m', '’re', 'through', 'first', 'n‘t', 'seem', 'meanwhile', 'forty', 'STEPWORD3', 'not', 'perhaps', 'two', 'former', 'those', 'doing', 'over', \"'d\", 'behind', 'back', 'sixty', 'none', 'ca', \"'m\", 'somewhere', 'these', 'more', 'it', 'himself', 'have', 'we', 'off', 'do', 'full', 'any', 'becoming', 'however', 'itself', 'one', 'which', 'enough', 'anyhow', 'as', 'rather', 'move', 'nobody', 'made', 'third', 'anywhere', 'well', 'various', 'six', 'was', 'against', 'several', 'moreover', 'get', 'thus', 'often', 'whatever', 'whereas', 'hers', 'than', 'empty', 'due', '’ve', 'cannot', 'indeed', 'above', 'name', 'thereby', 'our', 'top', 'when', 'every', 'also', 'am', 'thereafter', 'here', 'unless', 'herself', 'just', 'all', 'still', 'something', 'together', 'otherwise', 'a', 'them', 'anyone', 'along', 'sometime', 'using', 'seemed', 'twelve', 'quite', 'beside', 'own', 'so', 'onto', 'another', 'myself', 'see', 'most', 'next', 'are', 'why', 'around', 'their', 'side', 'would', 'STEPWORD4', '‘d', 'therein', 'who', 'across', 'hereupon', 'before', 'five', 'nine', 'at', 'STEPWORD2', 'everywhere', 'mine', 'because', 'latter', 'such', 'by', 'four', 'while', 'much', 'else', 'fifteen', 'ours', 'very', '’d', 'further', 'only', \"'ve\", 'nothing', 'say', 'toward', 'throughout', 'within', \"n't\", 'eleven', 'via', 'ourselves', '‘re', '‘s', 'nowhere', 'or', '‘ll', 'whose', 'sometimes', 'call', 'yourself', '‘ve', 'might', 'out', 'about', 'least', 'in', 'should', 'give', 'will', 'few', 'what', 'they', 'yourselves', 'both', 'except', 'to', 'been', 'where', 'being', 'were', 'once', 'besides', 'does', 'amount', 'beyond', 'neither', 'this', 'hereby', '’m', 'eight', '’s', 'for', 'has', 'each', 'us', 'thru', 'hence', 'per', 'whenever', 'he', 'seems', '’ll', 'whoever', 'since', 'fifty', 'whither', 'had', 'thence', 'please', 'that', 'up', 'down', 'during', 'until', 'his', 'take', \"'ll\"}\n"
     ]
    }
   ],
   "source": [
    "nlp.Defaults.stop_words -= {\"always\",\"never\",\"between\",\"becomes\"}\n",
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7286e0c",
   "metadata": {},
   "source": [
    "# TASK 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d0ca73",
   "metadata": {},
   "source": [
    "##### Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed1c72e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PADUA HIGH SCHOOL - DAY\n",
      "Revision November 12, 1997\n",
      "I hope dinner's ready because I only have ten minutes before Mrs. Johnson squirts out a screamer.\n",
      "He grabs the mail and rifles through it, as he bends down to kiss Sharon on the cheek.\n",
      "MICHAEL- C'mon. I'm supposed to give you the tour. They head out of the office\n",
      "MICHAEL (continuing)- So -- which Dakota you from?\n",
      "          \n",
      "                                 \n",
      "\n",
      "PADUA\n",
      "HIGH\n",
      "SCHOOL\n",
      "-\n",
      "DAY\n",
      "\n",
      "\n",
      "Revision\n",
      "November\n",
      "12\n",
      ",\n",
      "1997\n",
      "\n",
      "\n",
      "I\n",
      "hope\n",
      "dinner\n",
      "'s\n",
      "ready\n",
      "because\n",
      "I\n",
      "only\n",
      "have\n",
      "ten\n",
      "minutes\n",
      "before\n",
      "Mrs.\n",
      "Johnson\n",
      "squirts\n",
      "out\n",
      "a\n",
      "screamer\n",
      ".\n",
      "\n",
      "\n",
      "He\n",
      "grabs\n",
      "the\n",
      "mail\n",
      "and\n",
      "rifles\n",
      "through\n",
      "it\n",
      ",\n",
      "as\n",
      "he\n",
      "bends\n",
      "down\n",
      "to\n",
      "kiss\n",
      "Sharon\n",
      "on\n",
      "the\n",
      "cheek\n",
      ".\n",
      "\n",
      "\n",
      "MICHAEL-\n",
      "C'm\n",
      "on\n",
      ".\n",
      "I\n",
      "'m\n",
      "supposed\n",
      "to\n",
      "give\n",
      "you\n",
      "the\n",
      "tour\n",
      ".\n",
      "They\n",
      "head\n",
      "out\n",
      "of\n",
      "the\n",
      "office\n",
      "\n",
      "\n",
      "MICHAEL\n",
      "(\n",
      "continuing)-\n",
      "So\n",
      "--\n",
      "which\n",
      "Dakota\n",
      "you\n",
      "from\n",
      "?\n",
      "\n",
      "          \n",
      "                                 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"Raw_data_for_analysis.txt\")\n",
    "contents = file.read()                                    \n",
    "contents = contents[:]\n",
    "\n",
    "print(contents)\n",
    "\n",
    "text_combined = str(contents)\n",
    "doc = nlp(text_combined) \n",
    "\n",
    "for token in doc:\n",
    "    print(token)\n",
    "len(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbba43a5",
   "metadata": {},
   "source": [
    "##### Removing stop-words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "785af3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "32c0df60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      "Filtered Sentence: [PADUA, HIGH, SCHOOL, -, DAY, \n",
      ", Revision, November, 12, ,, 1997, \n",
      ", hope, dinner, ready, minutes, Mrs., Johnson, squirts, screamer, ., \n",
      ", grabs, mail, rifles, ,, bends, kiss, Sharon, cheek, ., \n",
      ", MICHAEL-, C'm, ., supposed, tour, ., head, office, \n",
      ", MICHAEL, (, continuing)-, --, Dakota, ?, \n",
      "          \n",
      "                                 \n",
      "]\n"
     ]
    }
   ],
   "source": [
    "filtered_file=[]                                          \n",
    "doc = nlp(text_combined)\n",
    "\n",
    "for word in doc:                                  \n",
    "    if word.is_stop==False:                \n",
    "        filtered_file.append(word)                   \n",
    "print(\"\\n \\nFiltered Sentence:\",filtered_file)         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641f028a",
   "metadata": {},
   "source": [
    "##### Lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "12191564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PADUA ----- PADUA\n",
      "HIGH ----- HIGH\n",
      "SCHOOL ----- SCHOOL\n",
      "- ----- -\n",
      "DAY ----- DAY\n",
      "\n",
      " ----- \n",
      "\n",
      "Revision ----- Revision\n",
      "November ----- November\n",
      "12 ----- 12\n",
      ", ----- ,\n",
      "1997 ----- 1997\n",
      "\n",
      " ----- \n",
      "\n",
      "hope ----- hope\n",
      "dinner ----- dinner\n",
      "ready ----- ready\n",
      "minutes ----- minute\n",
      "Mrs. ----- Mrs.\n",
      "Johnson ----- Johnson\n",
      "squirts ----- squirt\n",
      "screamer ----- screamer\n",
      ". ----- .\n",
      "\n",
      " ----- \n",
      "\n",
      "grabs ----- grab\n",
      "mail ----- mail\n",
      "rifles ----- rifle\n",
      ", ----- ,\n",
      "bends ----- bend\n",
      "kiss ----- kiss\n",
      "Sharon ----- Sharon\n",
      "cheek ----- cheek\n",
      ". ----- .\n",
      "\n",
      " ----- \n",
      "\n",
      "MICHAEL- ----- michael-\n",
      "C'm ----- come\n",
      ". ----- .\n",
      "supposed ----- suppose\n",
      "tour ----- tour\n",
      ". ----- .\n",
      "head ----- head\n",
      "office ----- office\n",
      "\n",
      " ----- \n",
      "\n",
      "MICHAEL ----- MICHAEL\n",
      "( ----- (\n",
      "continuing)- ----- continuing)-\n",
      "-- ----- --\n",
      "Dakota ----- Dakota\n",
      "? ----- ?\n",
      "\n",
      "          \n",
      "                                 \n",
      " ----- \n",
      "          \n",
      "                                 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in filtered_file:                \n",
    "    print(word.text,\"-----\",word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee1f04d",
   "metadata": {},
   "source": [
    "##### Parts OF Speech (POS) Tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a2b7fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PADUA \t PROPN \t NNP\n",
      "HIGH \t PROPN \t NNP\n",
      "SCHOOL \t PROPN \t NNP\n",
      "- \t PUNCT \t HYPH\n",
      "DAY \t PROPN \t NNP\n",
      "\n",
      " \t SPACE \t _SP\n",
      "Revision \t PROPN \t NNP\n",
      "November \t PROPN \t NNP\n",
      "12 \t NUM \t CD\n",
      ", \t PUNCT \t ,\n",
      "1997 \t NUM \t CD\n",
      "\n",
      " \t SPACE \t _SP\n",
      "I \t PRON \t PRP\n",
      "hope \t VERB \t VBP\n",
      "dinner \t NOUN \t NN\n",
      "'s \t PART \t POS\n",
      "ready \t ADJ \t JJ\n",
      "because \t SCONJ \t IN\n",
      "I \t PRON \t PRP\n",
      "only \t ADV \t RB\n",
      "have \t VERB \t VBP\n",
      "ten \t NUM \t CD\n",
      "minutes \t NOUN \t NNS\n",
      "before \t SCONJ \t IN\n",
      "Mrs. \t PROPN \t NNP\n",
      "Johnson \t PROPN \t NNP\n",
      "squirts \t VERB \t VBZ\n",
      "out \t ADP \t RP\n",
      "a \t DET \t DT\n",
      "screamer \t NOUN \t NN\n",
      ". \t PUNCT \t .\n",
      "\n",
      " \t SPACE \t _SP\n",
      "He \t PRON \t PRP\n",
      "grabs \t VERB \t VBZ\n",
      "the \t DET \t DT\n",
      "mail \t NOUN \t NN\n",
      "and \t CCONJ \t CC\n",
      "rifles \t NOUN \t NNS\n",
      "through \t ADP \t IN\n",
      "it \t PRON \t PRP\n",
      ", \t PUNCT \t ,\n",
      "as \t SCONJ \t IN\n",
      "he \t PRON \t PRP\n",
      "bends \t VERB \t VBZ\n",
      "down \t ADP \t RP\n",
      "to \t PART \t TO\n",
      "kiss \t VERB \t VB\n",
      "Sharon \t PROPN \t NNP\n",
      "on \t ADP \t IN\n",
      "the \t DET \t DT\n",
      "cheek \t NOUN \t NN\n",
      ". \t PUNCT \t .\n",
      "\n",
      " \t SPACE \t _SP\n",
      "MICHAEL- \t X \t ADD\n",
      "C'm \t VERB \t VBP\n",
      "on \t ADP \t RP\n",
      ". \t PUNCT \t .\n",
      "I \t PRON \t PRP\n",
      "'m \t AUX \t VBP\n",
      "supposed \t VERB \t VBN\n",
      "to \t PART \t TO\n",
      "give \t VERB \t VB\n",
      "you \t PRON \t PRP\n",
      "the \t DET \t DT\n",
      "tour \t NOUN \t NN\n",
      ". \t PUNCT \t .\n",
      "They \t PRON \t PRP\n",
      "head \t VERB \t VBP\n",
      "out \t ADP \t IN\n",
      "of \t ADP \t IN\n",
      "the \t DET \t DT\n",
      "office \t NOUN \t NN\n",
      "\n",
      " \t SPACE \t _SP\n",
      "MICHAEL \t PROPN \t NNP\n",
      "( \t PUNCT \t -LRB-\n",
      "continuing)- \t NOUN \t NNS\n",
      "So \t ADV \t RB\n",
      "-- \t PUNCT \t :\n",
      "which \t PRON \t WDT\n",
      "Dakota \t PROPN \t NNP\n",
      "you \t PRON \t PRP\n",
      "from \t ADP \t IN\n",
      "? \t PUNCT \t .\n",
      "\n",
      "          \n",
      "                                 \n",
      " \t SPACE \t _SP\n"
     ]
    }
   ],
   "source": [
    "file = open(\"Raw_data_for_analysis.txt\")\n",
    "\n",
    "contents = file.read()                                 \n",
    "text = str(contents)                                              \n",
    "doc = nlp(text)\n",
    "\n",
    "for word in doc:          \n",
    "    print(word.text,'\\t',word.pos_,'\\t',word.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faac9779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
